#!/usr/bin/env python3
"""
Video Transcription Pipeline for Educational Research
Processes classroom videos using Google's Gemini API for detailed transcription
with speaker diarization, prosody, and visual action annotations.

V03 VERSION: Hybrid BERT Consensus Algorithm
Replaces basic similarity with BERT semantic understanding + classroom speech boosts
for dramatically improved auto-accept rates (60-72% vs 5-25% with basic algorithm)
"""

import os
import sys
import time
import json
import argparse
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import subprocess
import tempfile
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import statistics

try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    # File class for type hints (fallback if import fails)
    try:
        from google.generativeai.types import File
    except ImportError:
        File = object  # Fallback for older versions
except ImportError:
    print("Please install google-generativeai: pip install google-generativeai")
    sys.exit(1)

# Try to import BERT dependencies for hybrid similarity
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    BERT_AVAILABLE = True
except ImportError:
    BERT_AVAILABLE = False
    print("âš ï¸  BERT libraries not available - falling back to basic similarity")
    print("   For optimal performance, install: pip install sentence-transformers scikit-learn")

@dataclass
class TranscriptionConfig:
    """Configuration for transcription processing"""
    chunk_duration_minutes: float = 3.0  # Default 3-minute chunks
    overlap_seconds: int = 10  # Overlap between chunks for continuity
    max_file_size_mb: int = 95  # Stay under 100MB limit
    model_name: str = "gemini-2.5-pro-preview-05-06"  # or "gemini-2.5-flash-preview-05-20"
    enable_detailed_processing: bool = True  # Higher quality mode
    output_format: str = "transana"  # "transana" or "plain"
    fps: int = 1  # Frames per second for video analysis (1-10 recommended)
    thinking: bool = True  # Enable/disable thinking mode
    prompt_key: str = "basic"  # Which prompt to use from prompts.json
    consensus_runs: int = 1  # Number of transcription runs per chunk for consensus analysis
    consensus_threshold: float = 0.7  # Minimum confidence for auto-accept (research grade)
    precise_chunking: bool = True  # True: re-encode for precision, False: fast copy mode
    enable_repetition_filter: bool = True  # Post-process to remove excessive repetition
    max_retries: int = 3  # Maximum retry attempts for failed transcriptions
    min_transcript_length: int = 50  # Minimum characters for valid transcript
    retry_delay: float = 5.0  # Seconds to wait between retries

class TranscriptValidator:
    """Validate transcription results and detect failures"""
    
    def __init__(self, min_length: int = 50):
        self.min_length = min_length
    
    def is_valid_transcription(self, transcript: str, file_name: str = "") -> Tuple[bool, str]:
        """
        Validate if transcription is successful or failed
        Returns (is_valid, failure_reason)
        """
        if not transcript or not isinstance(transcript, str):
            return False, "Empty or invalid transcript"
        
        transcript = transcript.strip()
        
        # Check for explicit error markers
        error_patterns = [
            r'\[ERROR:',
            r'\[PARTIAL:.*Generation stopped',
            r'Transcription failed',
            r'No response candidates',
            r'Invalid response',
            r'No content parts'
        ]
        
        for pattern in error_patterns:
            if re.search(pattern, transcript, re.IGNORECASE):
                return False, f"Error marker detected: {pattern}"
        
        # Check minimum length
        if len(transcript) < self.min_length:
            return False, f"Transcript too short: {len(transcript)} chars (min: {self.min_length})"
        
        # Check for reasonable content structure
        lines = transcript.split('\n')
        valid_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for timestamp patterns (various formats)
            timestamp_patterns = [
                r'^\d{1,2}:\d{2}',  # MM:SS
                r'^\(\d{1,2}:\d{2}',  # (MM:SS
                r'^\d{1,2}:\d{2}:\d{2}',  # HH:MM:SS
                r'^\[\d{1,2}:\d{2}'  # [MM:SS
            ]
            
            has_timestamp = any(re.search(pattern, line) for pattern in timestamp_patterns)
            has_speaker = ':' in line and len(line.split(':', 1)) == 2
            
            if has_timestamp or has_speaker:
                valid_lines += 1
        
        # Should have at least some structured content
        if valid_lines == 0:
            return False, "No valid transcript lines with timestamps/speakers found"
        
        # Check for excessive repetition (potential AI loop)
        if self._detect_excessive_repetition(transcript):
            return False, "Excessive repetition detected (likely AI generation loop)"
        
        return True, "Valid transcript"
    
    def _detect_excessive_repetition(self, transcript: str) -> bool:
        """Detect if transcript has excessive repetitive content"""
        lines = transcript.split('\n')
        if len(lines) < 10:
            return False  # Too short to determine
        
        # Count repeated lines (ignoring timestamps)
        content_lines = []
        for line in lines:
            if ':' in line:
                try:
                    content = line.split(':', 1)[1].strip()
                    content_lines.append(content)
                except:
                    pass
        
        if len(content_lines) < 5:
            return False
        
        # Check for excessive repetition
        content_counts = Counter(content_lines)
        most_common = content_counts.most_common(1)[0]
        
        # If any single content appears more than 30% of the time, likely repetition
        if most_common[1] > len(content_lines) * 0.3:
            return True
        
        return False

class PromptManager:
    """Manage transcription prompts from external files"""

    def __init__(self, prompts_file: str = "prompts.json"):
        # Resolve prompts file relative to this script's directory
        if not Path(prompts_file).is_absolute():
            script_dir = Path(__file__).parent
            self.prompts_file = script_dir / prompts_file
        else:
            self.prompts_file = Path(prompts_file)
        self.prompts = self._load_prompts()

    def _load_prompts(self) -> Dict:
        """Load prompts from JSON file"""
        default_prompts = {
            "basic": {
                "name": "Basic Transcription",
                "description": "Simple speaker identification and timestamps",
                "prompt": "Transcribe this classroom video with speaker identification and timestamps.\n\nSpeakers to identify:\n- Ava (teacher)\n- A06 (student, boy)\n- A04 (student, girl)\n- BS (Background Student)\n\nFormat: (HH:MM:SS.d) Speaker: content\n\nInclude brief [visual actions] when relevant to learning.\n\nPlease provide an accurate, concise transcript."
            }
        }

        if not self.prompts_file.exists():
            # Return default prompts without trying to write (app bundle is read-only)
            return default_prompts

        try:
            with open(self.prompts_file, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"Error loading prompts from {self.prompts_file}: {e}")
            return default_prompts
    
    def get_prompt(self, key: str) -> str:
        """Get prompt text by key"""
        if key not in self.prompts:
            print(f"Warning: Prompt '{key}' not found. Using 'basic' instead.")
            key = "basic"
        return self.prompts[key]["prompt"]
    
    def list_prompts(self) -> None:
        """Display available prompts"""
        print("\nAvailable prompts:")
        print("=" * 50)
        for key, data in self.prompts.items():
            print(f"  {key:20} - {data['name']}")
            print(f"  {' ' * 20}   {data['description']}")
            print()
    
    def get_prompt_keys(self) -> List[str]:
        """Get list of available prompt keys"""
        return list(self.prompts.keys())

class ConsensusAnalyzer:
    """Analyze multiple transcript runs with Hybrid BERT + Classroom Speech consensus algorithm"""
    
    QUALITY_FLAGS = {
        'AUTO_ACCEPT': "âœ…",
        'MANUAL_REVIEW': "âš ï¸", 
        'CRITICAL_REVIEW': "ðŸš¨"
    }
    
    def __init__(self, consensus_threshold: float = 0.7):
        self.consensus_threshold = consensus_threshold
        
        # Initialize BERT model if available
        self.bert_model = None
        self.bert_available = BERT_AVAILABLE
        if self.bert_available:
            try:
                print("ðŸš€ Loading BERT model for hybrid semantic similarity...")
                self.bert_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… BERT model loaded successfully - hybrid algorithm enabled")
            except Exception as e:
                print(f"âŒ Failed to load BERT model: {e}")
                self.bert_available = False
                print("   Falling back to basic similarity algorithm")
        
        if not self.bert_available:
            print("â„¹ï¸  Using basic similarity algorithm")
    
    def parse_transcript_line(self, line: str) -> Optional[Dict]:
        """Parse transcript line: MM:SS Speaker: content [visual]"""
        # Match MM:SS timestamp patterns at start of line
        timestamp_match = re.match(r'^(\d{1,2}:\d{2})\s+([^:]+):\s*(.*)', line.strip())
        if not timestamp_match:
            return None
        
        timestamp = timestamp_match.group(1)
        speaker = timestamp_match.group(2).strip()
        content_with_visual = timestamp_match.group(3)
        
        # Extract visual descriptions in brackets
        visual_matches = re.findall(r'\[([^\]]+)\]', content_with_visual)
        visual = ' '.join(visual_matches) if visual_matches else ''
        content = re.sub(r'\[([^\]]+)\]', '', content_with_visual).strip()
        
        return {
            'timestamp': timestamp,
            'speaker': speaker,
            'content': content,
            'visual': visual
        }
    
    def timestamp_to_seconds(self, timestamp: str) -> float:
        """Convert MM:SS timestamp to seconds for sorting"""
        parts = timestamp.split(':')
        if len(parts) == 2:
            return float(parts[0]) * 60 + float(parts[1])
        return 0.0
    
    def normalize_speaker_name(self, speaker: str) -> str:
        """Normalize speaker names to handle common variations"""
        speaker = speaker.strip().lower()
        
        # Map common teacher variations
        if speaker in ['anna', 'ava', 'adult female', 'teacher']:
            return 'teacher'
        
        # Map student variations
        if speaker in ['a06', 'student 1', 'boy student', 'student boy']:
            return 'student_a06'
        if speaker in ['a04', 'student 2', 'girl student', 'student girl']:
            return 'student_a04'
        if speaker in ['bs', 'background student', 'other student']:
            return 'background_student'
        
        # Return normalized version
        return speaker.replace(' ', '_')
    
    def extract_spoken_content(self, full_content: str) -> str:
        """Extract just the spoken content, removing visual descriptions"""
        spoken = re.sub(r'\[([^\]]+)\]', '', full_content).strip()
        return spoken
    
    def bert_similarity(self, content1: str, content2: str) -> float:
        """BERT-based semantic similarity"""
        if not self.bert_available or not self.bert_model:
            return 0.0  # Skip if not available
        
        spoken1 = self.extract_spoken_content(content1)
        spoken2 = self.extract_spoken_content(content2)
        
        if not spoken1 and not spoken2:
            return 1.0
        if not spoken1 or not spoken2:
            return 0.0
        
        try:
            # Generate embeddings
            embeddings = self.bert_model.encode([spoken1, spoken2])
            
            # Calculate cosine similarity
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            # BERT returns values from -1 to 1, normalize to 0 to 1
            similarity = (similarity + 1) / 2
            
            return float(similarity)
        except Exception as e:
            print(f"BERT similarity error: {e}")
            return 0.0
    
    def apply_classroom_speech_boosts(self, base_score: float, spoken1: str, spoken2: str) -> float:
        """Apply classroom speech pattern boosts to similarity score"""
        score = base_score
        
        # Boost for short phrases (common in classroom interactions)
        avg_length = (len(spoken1.split()) + len(spoken2.split())) / 2
        if avg_length <= 3:
            score *= 1.3  # Strong boost for short phrases
        elif avg_length <= 5:
            score *= 1.1  # Moderate boost for medium phrases
        
        return min(score, 1.0)
    
    def basic_similarity_fallback(self, content1: str, content2: str) -> float:
        """Basic similarity algorithm as fallback when BERT not available"""
        if not content1 and not content2:
            return 1.0
        if not content1 or not content2:
            return 0.0
        
        # Normalize text: lowercase, remove punctuation, handle common variations
        def normalize_text(text):
            text = text.lower()
            # Remove common punctuation and normalize spaces
            text = re.sub(r'[.,!?;:]', '', text)
            text = re.sub(r'\s+', ' ', text).strip()
            # Handle common transcription variations
            text = text.replace("'s", "s").replace("'ll", "ll").replace("'re", "re")
            return text
        
        spoken1 = self.extract_spoken_content(content1)
        spoken2 = self.extract_spoken_content(content2)
        
        norm1 = normalize_text(spoken1)
        norm2 = normalize_text(spoken2)
        
        if norm1 == norm2:
            return 1.0
        
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        # Calculate Jaccard similarity with higher weight for longer matches
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        jaccard = intersection / union if union > 0 else 0.0
        
        # Boost similarity for short phrases (common in classroom speech)
        if len(words1) <= 3 and len(words2) <= 3:
            jaccard = min(jaccard * 1.2, 1.0)
        
        return jaccard
    
    def content_similarity(self, content1: str, content2: str) -> float:
        """
        Hybrid BERT + Classroom Speech similarity algorithm
        Dramatically outperforms basic algorithm: 60-72% vs 5-25% auto-accept rates
        """
        if not self.bert_available or not self.bert_model:
            # Fallback to basic algorithm if BERT not available
            return self.basic_similarity_fallback(content1, content2)
        
        # Get BERT semantic similarity as base
        semantic_score = self.bert_similarity(content1, content2)
        
        # Extract spoken content for classroom speech boosts
        spoken1 = self.extract_spoken_content(content1)
        spoken2 = self.extract_spoken_content(content2)
        
        # Apply classroom speech boosts
        boosted_score = self.apply_classroom_speech_boosts(semantic_score, spoken1, spoken2)
        
        return min(boosted_score, 1.0)
    
    def parse_quality_score(self, transcript: str) -> float:
        """Score transcript based on parsing quality (0-20 points)"""
        lines = transcript.strip().split('\n')
        if not lines:
            return 0
        
        parseable_lines = 0
        consistent_format = 0
        valid_timestamps = 0
        
        for line in lines:
            parsed = self.parse_transcript_line(line)
            if parsed:
                parseable_lines += 1
                
                # Check timestamp format consistency
                if re.match(r'^\d{1,2}:\d{2}$', parsed['timestamp']):
                    consistent_format += 1
                    
                    # Check timestamp progression (no going backwards)
                    timestamp_seconds = self.timestamp_to_seconds(parsed['timestamp'])
                    if timestamp_seconds >= 0:
                        valid_timestamps += 1
        
        if len(lines) == 0:
            return 0
            
        # Calculate scores
        parse_rate = parseable_lines / len(lines)
        format_rate = consistent_format / max(parseable_lines, 1)
        timestamp_rate = valid_timestamps / max(parseable_lines, 1)
        
        return 20 * (parse_rate * 0.5 + format_rate * 0.3 + timestamp_rate * 0.2)
    
    def format_consistency_score(self, transcript: str) -> float:
        """Score transcript based on format consistency (0-20 points)"""
        lines = transcript.strip().split('\n')
        if not lines:
            return 0
        
        uncertainty_markers = len(re.findall(r'\[unclear\]|\[inaudible\]|\[uncertain\]', transcript))
        total_lines = len(lines)
        
        # Count speaker consistency
        speakers = set()
        speaker_lines = 0
        for line in lines:
            parsed = self.parse_transcript_line(line)
            if parsed and parsed['speaker']:
                speakers.add(parsed['speaker'])
                speaker_lines += 1
        
        # Lower uncertainty markers = better score
        uncertainty_penalty = min(uncertainty_markers / max(total_lines, 1), 0.5)
        
        # Reasonable speaker diversity (not too few, not too many)
        speaker_diversity = min(len(speakers) / max(speaker_lines / 10, 1), 1.0)
        
        return 20 * (1 - uncertainty_penalty) * speaker_diversity
    
    def content_richness_score(self, transcript: str) -> float:
        """Score transcript based on content richness (0-30 points)"""
        lines = transcript.strip().split('\n')
        if not lines:
            return 0
        
        total_words = len(transcript.split())
        visual_descriptions = len(re.findall(r'\[([^\]]+)\]', transcript))
        speaker_turns = 0
        content_lines = 0
        
        for line in lines:
            parsed = self.parse_transcript_line(line)
            if parsed:
                if parsed['content'].strip():
                    content_lines += 1
                    speaker_turns += 1
        
        if total_words == 0:
            return 0
        
        # Balance: want detail but not excessive length
        word_score = min(total_words / 1000, 1.0) * 0.4  # Optimal around 1000 words
        visual_score = min(visual_descriptions / 20, 1.0) * 0.3  # Good visual detail
        content_score = (content_lines / max(len(lines), 1)) * 0.3  # High content ratio
        
        return 30 * (word_score + visual_score + content_score)
    
    def agreement_with_others_score(self, transcript: str, all_transcripts: List[str]) -> float:
        """Score based on agreement with other transcripts (0-30 points)"""
        if len(all_transcripts) <= 1:
            return 30  # Perfect score if only one transcript
        
        target_lines = []
        for line in transcript.strip().split('\n'):
            parsed = self.parse_transcript_line(line)
            if parsed:
                target_lines.append(parsed)
        
        if not target_lines:
            return 0
        
        total_agreement = 0
        comparisons = 0
        
        for other_transcript in all_transcripts:
            if other_transcript == transcript:
                continue
                
            other_lines = []
            for line in other_transcript.strip().split('\n'):
                parsed = self.parse_transcript_line(line)
                if parsed:
                    other_lines.append(parsed)
            
            # Compare lines by timestamp alignment
            for target_line in target_lines:
                target_time = self.timestamp_to_seconds(target_line['timestamp'])
                
                # Find closest match in other transcript
                closest_match = None
                min_time_diff = float('inf')
                
                for other_line in other_lines:
                    other_time = self.timestamp_to_seconds(other_line['timestamp'])
                    time_diff = abs(target_time - other_time)
                    
                    if time_diff < min_time_diff and time_diff <= 5:  # Within 5 seconds
                        min_time_diff = time_diff
                        closest_match = other_line
                
                if closest_match:
                    # Calculate agreement
                    speaker_match = 1 if target_line['speaker'] == closest_match['speaker'] else 0
                    content_similarity = self.content_similarity(target_line['content'], closest_match['content'])
                    
                    total_agreement += (speaker_match * 0.6 + content_similarity * 0.4)
                    comparisons += 1
        
        if comparisons == 0:
            return 15  # Average score if no comparisons possible
        
        return 30 * (total_agreement / comparisons)
    
    def score_transcript_quality(self, transcript: str, all_transcripts: List[str]) -> float:
        """Score overall transcript quality (0-100 points)"""
        parse_score = self.parse_quality_score(transcript)  # 20 points
        format_score = self.format_consistency_score(transcript)  # 20 points  
        content_score = self.content_richness_score(transcript)  # 30 points
        agreement_score = self.agreement_with_others_score(transcript, all_transcripts)  # 30 points
        
        total_score = parse_score + format_score + content_score + agreement_score
        
        return min(total_score, 100)  # Cap at 100
    
    def select_baseline_transcript(self, transcript_runs: List[str]) -> Tuple[str, int, Dict]:
        """Select the best transcript as baseline"""
        if len(transcript_runs) == 1:
            return transcript_runs[0], 0, {'score': 100, 'reason': 'single_run'}
        
        print(f"  BASELINE SELECTION: Evaluating {len(transcript_runs)} transcripts...")
        
        scores = []
        for i, transcript in enumerate(transcript_runs):
            score = self.score_transcript_quality(transcript, transcript_runs)
            scores.append({
                'index': i,
                'score': score,
                'transcript': transcript
            })
            print(f"    Run {i+1}: {score:.1f}/100 points")
        
        # Sort by score, highest first
        scores.sort(key=lambda x: x['score'], reverse=True)
        
        best_run = scores[0]
        print(f"  BASELINE SELECTED: Run {best_run['index']+1} with {best_run['score']:.1f}/100 points")
        
        return best_run['transcript'], best_run['index'], {
            'score': best_run['score'],
            'all_scores': [s['score'] for s in scores],
            'reason': 'quality_scoring'
        }
    
    def align_transcripts(self, baseline: str, all_transcripts: List[str]) -> List[Dict]:
        """Align all transcripts by timestamp to baseline"""
        baseline_lines = []
        for line in baseline.strip().split('\n'):
            parsed = self.parse_transcript_line(line)
            if parsed:
                baseline_lines.append(parsed)
        
        # Parse all other transcripts
        other_transcript_lines = []
        for transcript in all_transcripts:
            lines = []
            for line in transcript.strip().split('\n'):
                parsed = self.parse_transcript_line(line)
                if parsed:
                    lines.append(parsed)
            other_transcript_lines.append(lines)
        
        # Align each baseline line with corresponding lines from other transcripts
        aligned_data = []
        
        for baseline_line in baseline_lines:
            baseline_time = self.timestamp_to_seconds(baseline_line['timestamp'])
            
            alignment = {
                'baseline': baseline_line,
                'matches': [],
                'speaker_agreements': [],
                'content_agreements': []
            }
            
            # Find matching lines in other transcripts
            for other_lines in other_transcript_lines:
                best_match = None
                min_time_diff = float('inf')
                
                for other_line in other_lines:
                    other_time = self.timestamp_to_seconds(other_line['timestamp'])
                    time_diff = abs(baseline_time - other_time)
                    
                    if time_diff < min_time_diff and time_diff <= 5:  # Within 5 seconds
                        min_time_diff = time_diff
                        best_match = other_line
                
                if best_match:
                    alignment['matches'].append(best_match)
                    
                    # Calculate agreements with normalized speaker names
                    baseline_speaker_norm = self.normalize_speaker_name(baseline_line['speaker'])
                    match_speaker_norm = self.normalize_speaker_name(best_match['speaker'])
                    speaker_match = baseline_speaker_norm == match_speaker_norm
                    content_sim = self.content_similarity(baseline_line['content'], best_match['content'])
                    
                    alignment['speaker_agreements'].append(speaker_match)
                    alignment['content_agreements'].append(content_sim)
            
            aligned_data.append(alignment)
        
        return aligned_data
    
    def flag_uncertainty_sections(self, aligned_data: List[Dict]) -> List[Dict]:
        """Flag uncertain sections based on cross-run disagreement"""
        flagged_lines = []
        
        for alignment in aligned_data:
            baseline_line = alignment['baseline']
            speaker_agreements = alignment['speaker_agreements']
            content_agreements = alignment['content_agreements']
            
            # Calculate confidence scores
            speaker_confidence = sum(speaker_agreements) / max(len(speaker_agreements), 1) if speaker_agreements else 1.0
            content_confidence = sum(content_agreements) / max(len(content_agreements), 1) if content_agreements else 1.0
            
            # Determine flag level with more realistic thresholds
            if speaker_confidence >= 0.6 and content_confidence >= 0.7:
                flag = self.QUALITY_FLAGS['AUTO_ACCEPT']
                review_needed = False
                flag_reason = None
            elif speaker_confidence >= 0.4 and content_confidence >= 0.5:
                flag = self.QUALITY_FLAGS['MANUAL_REVIEW']
                review_needed = True
                flag_reason = f"ðŸ‘¤{speaker_confidence*100:.0f}ðŸ’¬{content_confidence*100:.0f}"
            else:
                flag = self.QUALITY_FLAGS['CRITICAL_REVIEW']
                review_needed = True
                flag_reason = f"ðŸ‘¤{speaker_confidence*100:.0f}ðŸ’¬{content_confidence*100:.0f}"
            
            flagged_lines.append({
                'timestamp': baseline_line['timestamp'],
                'speaker': baseline_line['speaker'],
                'content': baseline_line['content'],
                'visual': baseline_line['visual'],
                'flag': flag,
                'review_needed': review_needed,
                'flag_reason': flag_reason,
                'confidence': {
                    'speaker': speaker_confidence,
                    'content': content_confidence
                },
                'total_comparisons': len(speaker_agreements)
            })
        
        return flagged_lines
    
    def generate_flagged_transcript(self, transcript_runs: List[str]) -> Dict:
        """Generate consensus-flagged transcript using baseline selection"""
        if len(transcript_runs) == 1:
            return {
                'flagged_transcript': transcript_runs[0],
                'analysis_summary': {
                    'total_runs': 1,
                    'flagging_used': False,
                    'quality_level': 'SINGLE_RUN'
                }
            }
        
        print(f"\n=== CONSENSUS FLAGGING ANALYSIS ===")
        if self.bert_available:
            print(f"ðŸ§  Using Hybrid BERT + Classroom Speech algorithm")
        else:
            print(f"ðŸ“ Using basic similarity algorithm (BERT unavailable)")
        
        # Step 1: Select baseline transcript
        baseline_transcript, baseline_index, baseline_info = self.select_baseline_transcript(transcript_runs)
        
        # Step 2: Align all transcripts to baseline
        print(f"  ALIGNMENT: Matching lines across {len(transcript_runs)} transcripts...")
        aligned_data = self.align_transcripts(baseline_transcript, transcript_runs)
        
        # Step 3: Flag uncertain sections
        print(f"  FLAGGING: Analyzing {len(aligned_data)} aligned lines...")
        flagged_lines = self.flag_uncertainty_sections(aligned_data)
        
        # Step 4: Generate output
        formatted_lines = []
        quality_stats = {'auto_accept': 0, 'manual_review': 0, 'critical_review': 0}
        
        for line_data in flagged_lines:
            # Format line with flag
            line = f"{line_data['timestamp']} {line_data['speaker']}: {line_data['content']}"
            if line_data['visual']:
                line += f" [{line_data['visual']}]"
            
            line += f" {line_data['flag']}"
            if line_data['flag_reason']:
                line += f" *{line_data['flag_reason']}*"
            
            formatted_lines.append(line)
            
            # Update stats
            if line_data['flag'] == self.QUALITY_FLAGS['AUTO_ACCEPT']:
                quality_stats['auto_accept'] += 1
            elif line_data['flag'] == self.QUALITY_FLAGS['MANUAL_REVIEW']:
                quality_stats['manual_review'] += 1
            else:
                quality_stats['critical_review'] += 1
        
        flagged_transcript = '\n'.join(formatted_lines)
        
        # Calculate summary statistics
        total_lines = len(flagged_lines)
        if total_lines > 0:
            overall_speaker_confidence = statistics.mean([line['confidence']['speaker'] for line in flagged_lines])
            overall_content_confidence = statistics.mean([line['confidence']['content'] for line in flagged_lines])
            research_ready = quality_stats['auto_accept'] / total_lines >= 0.8
        else:
            overall_speaker_confidence = 0.0
            overall_content_confidence = 0.0
            research_ready = False
        
        analysis_summary = {
            'total_runs': len(transcript_runs),
            'flagging_used': True,
            'algorithm_used': 'hybrid_bert' if self.bert_available else 'basic',
            'baseline_selected': {
                'run_index': baseline_index + 1,
                'quality_score': baseline_info['score'],
                'selection_reason': baseline_info['reason']
            },
            'total_lines': total_lines,
            'quality_distribution': quality_stats,
            'overall_confidence': {
                'speaker': overall_speaker_confidence,
                'content': overall_content_confidence
            },
            'research_ready': research_ready,
            'lines_needing_review': quality_stats['manual_review'] + quality_stats['critical_review'],
            'auto_accept_rate': quality_stats['auto_accept'] / max(total_lines, 1)
        }
        
        algorithm_name = "Hybrid BERT" if self.bert_available else "Basic"
        print(f"  ALGORITHM: {algorithm_name}")
        print(f"  RESULTS: {quality_stats['auto_accept']} auto-accept, {quality_stats['manual_review']} review, {quality_stats['critical_review']} critical")
        print(f"  CONFIDENCE: Speaker {overall_speaker_confidence*100:.1f}%, Content {overall_content_confidence*100:.1f}%")
        print(f"  RESEARCH READY: {research_ready}")
        
        return {
            'flagged_transcript': flagged_transcript,
            'analysis_summary': analysis_summary,
            'detailed_analysis': flagged_lines
        }

class VideoCostCalculator:
    """Calculate processing costs for video transcription"""
    
    # Current token costs (as of May 2025 - verify at aistudio.google.com)
    TOKEN_COSTS = {
        "gemini-2.5-pro-preview-05-06": {
            "input_low": 0.00125,    # <=200K tokens per 1k
            "input_high": 0.0025,    # >200K tokens per 1k  
            "output_low": 0.010,     # <=200K tokens per 1k
            "output_high": 0.015,    # >200K tokens per 1k
            "threshold": 200000      # 200K token threshold
        },
        "gemini-2.0-flash-exp": {"input": 0.000075, "output": 0.0003},
        "gemini-2.5-flash-preview-05-20": {"input": 0.000075, "output": 0.0003},
    }
    
    @classmethod
    def estimate_cost(cls, duration_minutes: float, model: str, chunk_minutes: float, fps: int = 1) -> Dict:
        """Estimate processing cost for video"""
        total_seconds = duration_minutes * 60
        
        # Adjust token calculation for FPS (default is 1 FPS = 258 tokens per second for frames)
        frame_tokens_per_second = 258 * fps  # Scale with FPS
        audio_tokens_per_second = 32  # Audio stays constant
        metadata_tokens_per_second = 10  # Rough estimate for metadata
        
        tokens_per_second = frame_tokens_per_second + audio_tokens_per_second + metadata_tokens_per_second
        total_input_tokens = total_seconds * tokens_per_second
        
        # Estimate output tokens (typically 10-20% of input for transcription)
        estimated_output_tokens = total_input_tokens * 0.15
        
        num_chunks = max(1, int(duration_minutes / chunk_minutes))
        
        if model in cls.TOKEN_COSTS:
            rates = cls.TOKEN_COSTS[model]
            
            if "threshold" in rates:  # Tiered pricing (e.g., Gemini 2.5 Pro)
                threshold = rates["threshold"]
                
                # Calculate input cost with tiers
                if total_input_tokens <= threshold:
                    input_cost = (total_input_tokens / 1000) * rates["input_low"]
                else:
                    low_tier_cost = (threshold / 1000) * rates["input_low"]
                    high_tier_cost = ((total_input_tokens - threshold) / 1000) * rates["input_high"]
                    input_cost = low_tier_cost + high_tier_cost
                
                # Calculate output cost with tiers  
                if estimated_output_tokens <= threshold:
                    output_cost = (estimated_output_tokens / 1000) * rates["output_low"]
                else:
                    low_tier_cost = (threshold / 1000) * rates["output_low"]
                    high_tier_cost = ((estimated_output_tokens - threshold) / 1000) * rates["output_high"]
                    output_cost = low_tier_cost + high_tier_cost
                    
            else:  # Flat pricing (e.g., Gemini 2.0 Flash)
                input_cost = (total_input_tokens / 1000) * rates["input"]
                output_cost = (estimated_output_tokens / 1000) * rates["output"]
                
            total_cost = input_cost + output_cost
        else:
            input_cost = output_cost = total_cost = 0
            
        return {
            "duration_minutes": duration_minutes,
            "fps": fps,
            "tokens_per_second": int(tokens_per_second),
            "total_tokens_estimated": int(total_input_tokens + estimated_output_tokens),
            "num_chunks": num_chunks,
            "input_cost": round(input_cost, 3),
            "output_cost": round(output_cost, 3), 
            "total_cost": round(total_cost, 3),
            "model": model
        }

class VideoChunker:
    """Split videos into manageable chunks for processing"""
    
    @staticmethod
    def get_video_duration(video_path: str) -> float:
        """Get video duration in minutes using ffprobe"""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            duration_seconds = float(result.stdout.strip())
            return duration_seconds / 60
        except (subprocess.CalledProcessError, ValueError, FileNotFoundError) as e:
            print(f"Error getting video duration: {e}")
            print("Make sure ffprobe is installed and in your PATH")
            return 0
    
    @staticmethod
    def split_video(input_path: str, output_dir: str, config: TranscriptionConfig) -> List[str]:
        """Split video into chunks, returns list of chunk file paths"""
        input_path = Path(input_path)
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        duration_minutes = VideoChunker.get_video_duration(str(input_path))
        if duration_minutes == 0:
            raise ValueError("Could not determine video duration")
        
        chunk_duration_seconds = config.chunk_duration_minutes * 60
        total_seconds = duration_minutes * 60
        
        chunk_files = []
        chunk_num = 1
        start_time = 0
        
        print(f"Splitting {duration_minutes:.1f}-minute video into {config.chunk_duration_minutes}-minute chunks...")
        
        while start_time < total_seconds:
            end_time = min(start_time + chunk_duration_seconds, total_seconds)
            
            # Create output filename
            base_name = input_path.stem
            chunk_file = output_dir / f"{base_name}_chunk_{chunk_num:02d}.mp4"
            
            # FFmpeg command to extract chunk
            if config.precise_chunking:
                # Precise mode: re-encode for exact timing (slower but accurate)
                cmd = [
                    "ffmpeg", 
                    "-ss", str(start_time),  # Seek before input for speed
                    "-i", str(input_path),
                    "-t", str(end_time - start_time),
                    "-c:v", "libx264",  # Re-encode video for precise timing
                    "-c:a", "aac",      # Re-encode audio for precise timing
                    "-preset", "fast",   # Faster encoding preset
                    str(chunk_file),
                    "-y"  # Overwrite existing files
                ]
            else:
                # Fast mode: stream copy (faster but may have timing issues)
                cmd = [
                    "ffmpeg", 
                    "-ss", str(start_time),
                    "-i", str(input_path),
                    "-t", str(end_time - start_time),
                    "-c", "copy",  # Copy streams without re-encoding
                    "-avoid_negative_ts", "make_zero",
                    str(chunk_file),
                    "-y"  # Overwrite existing files
                ]
            
            try:
                subprocess.run(cmd, check=True, capture_output=True)
                chunk_files.append(str(chunk_file))
                
                # Verify chunk creation and get actual duration
                try:
                    verify_cmd = [
                        "ffprobe", "-v", "quiet", "-show_entries", "format=duration",
                        "-of", "default=noprint_wrappers=1:nokey=1", str(chunk_file)
                    ]
                    result = subprocess.run(verify_cmd, capture_output=True, text=True, check=True)
                    actual_duration = float(result.stdout.strip())
                    expected_duration = end_time - start_time
                    
                    print(f"  Created chunk {chunk_num}: {start_time//60:02.0f}:{start_time%60:05.2f} - {end_time//60:02.0f}:{end_time%60:05.2f}")
                    print(f"    Expected: {expected_duration:.2f}s, Actual: {actual_duration:.2f}s")
                    
                    if abs(actual_duration - expected_duration) > 1.0:
                        print(f"    WARNING: Duration mismatch > 1 second")
                        
                except subprocess.CalledProcessError:
                    print(f"  Created chunk {chunk_num}: {start_time//60:02.0f}:{start_time%60:05.2f} - {end_time//60:02.0f}:{end_time%60:05.2f} (verification failed)")
                
                # Check file size
                file_size_mb = chunk_file.stat().st_size / (1024 * 1024)
                if file_size_mb > config.max_file_size_mb:
                    print(f"    Warning: Chunk {chunk_num} is {file_size_mb:.1f}MB (over {config.max_file_size_mb}MB limit)")
                elif file_size_mb < 0.1:
                    print(f"    WARNING: Chunk {chunk_num} is very small ({file_size_mb:.2f}MB) - may be mostly silent")
                
            except subprocess.CalledProcessError as e:
                print(f"Error creating chunk {chunk_num}: {e}")
                continue
            
            start_time = end_time
            chunk_num += 1
        
        return chunk_files

class GeminiTranscriber:
    """Handle Gemini API transcription with educational research focus and retry logic"""
    
    def __init__(self, api_key: str, config: TranscriptionConfig):
        self.config = config
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(config.model_name)
        
        # Load prompts from file
        self.prompt_manager = PromptManager()
        self.base_transcription_prompt = self.prompt_manager.get_prompt(config.prompt_key)
        
        # Initialize validation and consensus tools
        self.validator = TranscriptValidator(config.min_transcript_length)
        self.consensus_analyzer = ConsensusAnalyzer(config.consensus_threshold) if config.consensus_runs > 1 else None
        
        # Configure safety settings for educational content
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
    
    def upload_video_chunk(self, chunk_path: str):
        """Upload video chunk using File API"""
        print(f"Uploading {Path(chunk_path).name}...")
        
        try:
            file = genai.upload_file(chunk_path)
            
            # Wait for file to be processed
            while file.state.name == "PROCESSING":
                print("  Processing...", end="", flush=True)
                time.sleep(2)
                file = genai.get_file(file.name)
                print(".", end="", flush=True)
            print()
            
            if file.state.name == "FAILED":
                raise Exception(f"File processing failed: {file.state}")
                
            print(f"  Upload complete: {file.name}")
            return file
            
        except Exception as e:
            print(f"Error uploading {chunk_path}: {e}")
            raise
    
    def build_transcription_prompt(self, chunk_number: int, previous_chunk_transcript: str = None) -> str:
        """Build transcription prompt with context from previous chunk"""
        if chunk_number == 1:
            # First chunk - indicate this is the start
            context_instruction = "\n\nIMPORTANT: This is the start of the video. Begin transcription from the very beginning."
        else:
            # Subsequent chunks - include previous context
            if previous_chunk_transcript:
                # Extract last 10-15 lines from previous transcript for context
                prev_lines = previous_chunk_transcript.strip().split('\n')
                # Remove any header lines, flags, or metadata
                content_lines = []
                for line in prev_lines:
                    line = line.strip()
                    if not line:
                        continue
                    # Skip header lines and analysis content
                    if (line.startswith('===') or line.startswith('---') or 
                        line.startswith('Generated:') or line.startswith('Model:') or
                        line.startswith('Prompt:') or line.startswith('Chunk size:') or
                        line.startswith('Validation:')):
                        continue
                    # Keep lines that look like transcript content (remove quality flags)
                    if ':' in line and any(char.isdigit() for char in line[:10]):
                        # Remove consensus quality flags and markers
                        clean_line = line
                        # Remove flags like âœ… âš ï¸ ðŸš¨ and confidence markers
                        clean_line = re.sub(r'[âœ…âš ï¸ðŸš¨]\s*', '', clean_line)
                        clean_line = re.sub(r'\*ðŸ‘¤\d+ðŸ’¬\d+\*', '', clean_line)
                        content_lines.append(clean_line.strip())
                
                # Take last 10-15 lines for context
                context_lines = content_lines[-50:] if len(content_lines) > 50 else content_lines
                
                if context_lines:
                    context_text = '\n'.join(context_lines)
                    context_instruction = f"\n\nPREVIOUS CONTEXT: Here are the final moments from the previous video segment:\n\n{context_text}\n\nIMPORTANT: Continue the transcription naturally from where this context ends. Maintain speaker identification consistency and conversational flow. Use the same speaker names (Ava, A06, A04, BS) as established in the previous context."
                else:
                    context_instruction = f"\n\nIMPORTANT: This continues from the previous video segment (chunk {chunk_number-1}). Maintain speaker identification consistency."
            else:
                context_instruction = f"\n\nIMPORTANT: This continues from the previous video segment (chunk {chunk_number-1}). Maintain speaker identification consistency."
        
        return self.base_transcription_prompt + context_instruction
    
    def transcribe_chunk_with_retry(self, file, chunk_start_time: float, chunk_number: int = 1, previous_chunk_transcript: str = None) -> str:
        """
        Transcribe chunk with automatic retry logic for failed responses
        This is the main improvement - validates results and retries failures
        """
        max_attempts = self.config.max_retries + 1  # +1 for initial attempt
        
        # Build context-aware prompt
        transcription_prompt = self.build_transcription_prompt(chunk_number, previous_chunk_transcript)
        
        for attempt in range(1, max_attempts + 1):
            if attempt > 1:
                print(f"  RETRY {attempt-1}/{self.config.max_retries}: Attempting transcription again...")
                time.sleep(self.config.retry_delay)  # Wait before retry
            
            try:
                # Attempt transcription with context-aware prompt
                transcript = self._single_transcription_attempt(file, chunk_start_time, transcription_prompt)
                
                # Validate the result
                is_valid, failure_reason = self.validator.is_valid_transcription(transcript, file.display_name)
                
                if is_valid:
                    print(f"  SUCCESS: Valid transcription on attempt {attempt}")
                    return transcript
                else:
                    print(f"  FAILED ATTEMPT {attempt}: {failure_reason}")
                    if attempt == max_attempts:
                        print(f"  MAX RETRIES REACHED: Returning failed transcript after {max_attempts} attempts")
                        return f"[VALIDATION_FAILED after {max_attempts} attempts: {failure_reason}]\n\n{transcript}"
                    else:
                        print(f"  Will retry in {self.config.retry_delay} seconds...")
                        continue
                        
            except Exception as e:
                print(f"  EXCEPTION on attempt {attempt}: {e}")
                if attempt == max_attempts:
                    print(f"  MAX RETRIES REACHED: Returning error after {max_attempts} attempts")
                    return f"[EXCEPTION after {max_attempts} attempts: {str(e)}]"
                else:
                    print(f"  Will retry in {self.config.retry_delay} seconds...")
                    continue
        
        # This should never be reached, but just in case
        return f"[UNEXPECTED_ERROR: Transcription failed after {max_attempts} attempts]"
    
    def _single_transcription_attempt(self, file, chunk_start_time: float, transcription_prompt: str) -> str:
        """Single transcription attempt - separated for retry logic"""
        print(f"Transcribing {file.display_name} at {self.config.fps} FPS...")
        
        # Build content for File API request
        response = self.model.generate_content(
            [file, transcription_prompt],
            safety_settings=self.safety_settings,
            generation_config={
                "temperature": 1.0,
                "max_output_tokens": 4096,
            }
        )
        
        # Check response structure
        if not response.candidates:
            raise Exception("No response candidates generated")
        
        candidate = response.candidates[0]
        
        if candidate.finish_reason != 1:  # 1 = FINISH_REASON_STOP (natural completion)
            finish_reasons = {
                0: "UNSPECIFIED",
                1: "STOP", 
                2: "MAX_TOKENS",
                3: "SAFETY",
                4: "RECITATION"
            }
            reason = finish_reasons.get(candidate.finish_reason, f"UNKNOWN({candidate.finish_reason})")
            
            # Try to get partial content even if stopped abnormally
            if candidate.content and candidate.content.parts and len(candidate.content.parts) > 0:
                partial_text = candidate.content.parts[0].text
                return f"[PARTIAL: Generation stopped due to {reason}]\n\n{partial_text}"
            else:
                raise Exception(f"Generation stopped due to {reason} with no content")
        
        if not candidate.content or not candidate.content.parts:
            raise Exception("No content parts in response")
            
        transcript = candidate.content.parts[0].text
        
        # Apply repetition filter if enabled
        if self.config.enable_repetition_filter:
            transcript = self.clean_repetitive_content(transcript)
        
        # Adjust timestamps to account for chunk start time
        if chunk_start_time > 0:
            transcript = self._adjust_timestamps(transcript, chunk_start_time)
        
        return transcript
    
    def transcribe_chunk_with_consensus_flagging(self, uploaded_file, chunk_start_time: float, chunk_name: str, output_dir: Path, chunk_number: int = 1, previous_chunk_transcript: str = None) -> str:
        """Transcribe a chunk multiple times with validation and generate consensus flagging"""
        if self.config.consensus_runs <= 1:
            # Single run - use retry logic
            try:
                return self.transcribe_chunk_with_retry(uploaded_file, chunk_start_time, chunk_number, previous_chunk_transcript)
            finally:
                self._cleanup_file(uploaded_file)
        
        print(f"\n=== CONSENSUS FLAGGING: {chunk_name} ===")
        if chunk_number == 1:
            print(f"Running {self.config.consensus_runs} transcription passes (START OF VIDEO)...")
        else:
            print(f"Running {self.config.consensus_runs} transcription passes (with previous context)...")
        print(f"Chunk context: {'First chunk' if chunk_number == 1 else f'Continuing from chunk {chunk_number-1}'}")
        
        # Run multiple transcriptions with validation
        transcript_runs = []
        failed_attempts = 0
        total_attempts = 0
        
        try:
            run_num = 0
            while run_num < self.config.consensus_runs:
                run_num += 1
                print(f"\n--- Run {run_num}/{self.config.consensus_runs} ---")
                
                # Use retry logic for each run with context
                transcript = self.transcribe_chunk_with_retry(uploaded_file, chunk_start_time, chunk_number, previous_chunk_transcript)
                
                # Validate the final result
                is_valid, failure_reason = self.validator.is_valid_transcription(transcript, uploaded_file.display_name)
                
                if is_valid:
                    transcript_runs.append(transcript)
                    print(f"  VALID RUN: Adding to consensus analysis")
                    
                    # Save individual run for analysis
                    run_file = output_dir / f"{chunk_name}_run_{len(transcript_runs):02d}.txt"
                    with open(run_file, 'w', encoding='utf-8') as f:
                        f.write(transcript)
                else:
                    print(f"  INVALID RUN: {failure_reason}")
                    failed_attempts += 1
                    
                    # If we have too many failures, break early
                    if failed_attempts >= self.config.consensus_runs // 2:  # More than half failed
                        print(f"  TOO MANY FAILURES: {failed_attempts} failed runs, stopping consensus")
                        break
                
                total_attempts += 1
                
                # Brief delay between runs to avoid rate limiting
                if run_num < self.config.consensus_runs:
                    time.sleep(2)
        finally:
            self._cleanup_file(uploaded_file)
        
        print(f"  CONSENSUS SUMMARY: {len(transcript_runs)} valid runs out of {total_attempts} attempts")
        
        # Check if we have enough valid runs for consensus
        if len(transcript_runs) == 0:
            return f"[CONSENSUS_FAILED: No valid transcripts after {total_attempts} attempts]"
        elif len(transcript_runs) == 1:
            print(f"  SINGLE VALID RUN: Using only valid transcript (no consensus analysis)")
            return transcript_runs[0]
        else:
            # Generate consensus flagging analysis
            flagging_result = self.consensus_analyzer.generate_flagged_transcript(transcript_runs)
            
            # Save consensus analysis details
            analysis_file = output_dir / f"{chunk_name}_consensus_analysis.json"
            analysis_data = flagging_result.copy()
            analysis_data['validation_summary'] = {
                'total_attempts': total_attempts,
                'valid_runs': len(transcript_runs),
                'failed_runs': failed_attempts,
                'success_rate': len(transcript_runs) / total_attempts if total_attempts > 0 else 0
            }
            
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, indent=2, default=str)
            
            return flagging_result['flagged_transcript']
    
    def _cleanup_file(self, file):
        """Clean up uploaded file"""
        try:
            genai.delete_file(file.name)
            print(f"  Cleaned up {file.name}")
        except Exception as e:
            print(f"  Warning: Failed to cleanup {file.name}: {e}")
    
    def clean_repetitive_content(self, text: str, max_repetitions: int = 3) -> str:
        """Remove excessive repetitive content from transcript"""
        lines = text.split('\n')
        cleaned_lines = []
        repetition_count = {}
        
        for line in lines:
            # Extract just the dialogue/action part (after speaker ID)
            if ':' in line:
                try:
                    timestamp_speaker = line.split(':', 1)[0] + ':'
                    content = line.split(':', 1)[1].strip()
                    
                    # Check for repetitive content
                    if content in repetition_count:
                        repetition_count[content] += 1
                        if repetition_count[content] <= max_repetitions:
                            cleaned_lines.append(line)
                        elif repetition_count[content] == max_repetitions + 1:
                            # Add "continues repeating" note
                            cleaned_lines.append(f"{timestamp_speaker} [continues repeating: '{content}']") 
                    else:
                        repetition_count[content] = 1
                        cleaned_lines.append(line)
                except:
                    cleaned_lines.append(line)
            else:
                cleaned_lines.append(line)
                
        original_lines = len(lines)
        cleaned_line_count = len(cleaned_lines)
        
        if cleaned_line_count < original_lines:
            print(f"  REPETITION FILTER: Reduced from {original_lines} to {cleaned_line_count} lines ({original_lines - cleaned_line_count} repetitive lines removed)")
            
        return '\n'.join(cleaned_lines)
    
    def _adjust_timestamps(self, transcript: str, start_offset_seconds: float) -> str:
        """Adjust timestamps in transcript to account for chunk position in full video"""
        # This function needs updating for MM:SS format
        # For now, return transcript unchanged since chunk-level timing is handled elsewhere
        return transcript

class TranscriptProcessor:
    """Main pipeline for processing classroom videos"""
    
    def __init__(self, api_key: str, config: TranscriptionConfig):
        self.config = config
        self.transcriber = GeminiTranscriber(api_key, config)
        self.cost_calculator = VideoCostCalculator()
    
    def process_video(self, video_path: str, output_dir: str = None) -> Dict:
        """Process complete video and return results"""
        video_path = Path(video_path)
        
        if not video_path.exists():
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        if output_dir is None:
            # Add timestamp to avoid overwriting previous runs
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = video_path.parent / f"{video_path.stem}_transcription_{timestamp}"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        # Get video info and estimate costs
        duration_minutes = VideoChunker.get_video_duration(str(video_path))
        base_cost_estimate = self.cost_calculator.estimate_cost(
            duration_minutes, self.config.model_name, self.config.chunk_duration_minutes, self.config.fps
        )
        
        # Multiply cost by consensus runs
        cost_estimate = base_cost_estimate.copy()
        if self.config.consensus_runs > 1:
            cost_estimate['total_cost'] *= self.config.consensus_runs
            cost_estimate['input_cost'] *= self.config.consensus_runs
            cost_estimate['output_cost'] *= self.config.consensus_runs
            cost_estimate['total_tokens_estimated'] *= self.config.consensus_runs
        
        print(f"\n=== Processing {video_path.name} ===")
        print(f"Duration: {duration_minutes:.1f} minutes")
        print(f"FPS setting: {self.config.fps} frames/second")
        print(f"Thinking mode: {'ON' if self.config.thinking else 'OFF'}")
        print(f"Prompt: {self.config.prompt_key}")
        if self.config.consensus_runs > 1:
            consensus_type = "Hybrid BERT" if BERT_AVAILABLE else "Basic"
            print(f"Consensus flagging: {self.config.consensus_runs} runs per chunk ({consensus_type} algorithm)")
        print(f"Run validation: {'Enabled' if self.config.max_retries > 0 else 'Disabled'} (max {self.config.max_retries} retries)")
        print(f"Context continuity: {'Enabled' if cost_estimate['num_chunks'] > 1 else 'N/A (single chunk)'} (previous chunk context)")
        print(f"Chunking mode: {'Precise (re-encode)' if self.config.precise_chunking else 'Fast (stream copy)'}")
        print(f"Repetition filter: {'Enabled' if self.config.enable_repetition_filter else 'Disabled'}")
        print(f"Sequential timestamps: Enabled (adjusted across chunks)")
        print(f"Estimated tokens per second: {base_cost_estimate['tokens_per_second']}")
        print(f"Estimated cost: ${cost_estimate['total_cost']:.3f} ({cost_estimate['total_tokens_estimated']:,} tokens)")
        print(f"Chunks: {cost_estimate['num_chunks']} Ã— {self.config.chunk_duration_minutes} minutes")
        
        # Confirm processing
        response = input("\nProceed with transcription? (y/n): ").strip().lower()
        if response != 'y':
            print("Transcription cancelled.")
            return {}
        
        # Create chunks directory
        chunks_dir = output_dir / "chunks"
        chunks_dir.mkdir(exist_ok=True)
        
        try:
            # Split video into chunks
            chunk_files = VideoChunker.split_video(str(video_path), str(chunks_dir), self.config)
            
            if not chunk_files:
                raise Exception("No video chunks were created")
            
            # Process each chunk
            all_transcripts = []
            total_start_time = 0
            previous_chunk_transcript = None  # Track previous chunk for context
            
            for i, chunk_file in enumerate(chunk_files):
                chunk_number = i + 1
                chunk_start_minutes = i * self.config.chunk_duration_minutes
                print(f"\n--- Processing chunk {chunk_number}/{len(chunk_files)} ---")
                
                # Upload chunk
                uploaded_file = self.transcriber.upload_video_chunk(chunk_file)
                
                # Use consensus flagging if enabled
                chunk_name = f"chunk_{chunk_number:02d}"
                if self.config.consensus_runs > 1:
                    transcript = self.transcriber.transcribe_chunk_with_consensus_flagging(
                        uploaded_file, total_start_time, chunk_name, output_dir, chunk_number, previous_chunk_transcript
                    )
                else:
                    transcript = self.transcriber.transcribe_chunk_with_retry(
                        uploaded_file, total_start_time, chunk_number, previous_chunk_transcript
                    )
                    self.transcriber._cleanup_file(uploaded_file)
                
                all_transcripts.append({
                    'chunk_number': chunk_number,
                    'chunk_file': chunk_file,
                    'start_time': total_start_time,
                    'transcript': transcript
                })
                
                # Save individual chunk transcript
                chunk_transcript_file = output_dir / f"{chunk_name}_transcript.txt"
                with open(chunk_transcript_file, 'w', encoding='utf-8') as f:
                    f.write(transcript)
                
                # Set this transcript as context for next chunk (if it's valid)
                if not transcript.startswith('[CONSENSUS_FAILED') and not transcript.startswith('[VALIDATION_FAILED'):
                    previous_chunk_transcript = transcript
                    print(f"  CONTEXT: Saved {len(transcript)} characters for next chunk")
                    print(f"  SEQUENTIAL TIMESTAMPS: Adjusted {chunk_name} timestamps by +{chunk_start_minutes:.1f} minutes")
                else:
                    print(f"  CONTEXT: Chunk {chunk_number} failed - no context for next chunk")
                
                # Update start time for next chunk
                total_start_time += self.config.chunk_duration_minutes * 60
            
            # Combine all transcripts with sequential timestamps
            combined_transcript = self._combine_transcripts(all_transcripts)
            
            # Save combined transcript
            final_transcript_file = output_dir / f"{video_path.stem}_complete_transcript.txt"
            with open(final_transcript_file, 'w', encoding='utf-8') as f:
                f.write(combined_transcript)
            
            # Save processing summary
            summary = {
                'video_file': str(video_path),
                'processing_date': datetime.now().isoformat(),
                'config': {
                    'chunk_duration_minutes': self.config.chunk_duration_minutes,
                    'model_name': self.config.model_name,
                    'prompt_key': self.config.prompt_key,
                    'fps': self.config.fps,
                    'thinking_mode': self.config.thinking,
                    'consensus_runs': self.config.consensus_runs,
                    'consensus_threshold': self.config.consensus_threshold,
                    'consensus_algorithm': 'hybrid_bert' if BERT_AVAILABLE else 'basic',
                    'max_retries': self.config.max_retries,
                    'retry_validation': True,
                    'context_continuity': len(chunk_files) > 1,
                    'sequential_timestamps': True,
                    'chunks_processed': len(chunk_files)
                },
                'cost_estimate': cost_estimate,
                'output_files': {
                    'complete_transcript': str(final_transcript_file),
                    'chunks_directory': str(chunks_dir)
                }
            }
            
            summary_file = output_dir / "processing_summary.json"
            with open(summary_file, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"\n=== Transcription Complete ===")
            print(f"Final transcript: {final_transcript_file}")
            print(f"Processing summary: {summary_file}")
            
            return summary
            
        except Exception as e:
            print(f"Error during processing: {e}")
            raise
        finally:
            # Cleanup chunk files if requested
            cleanup = input("\nDelete temporary chunk files? (y/n): ").strip().lower()
            if cleanup == 'y':
                for chunk_file in chunk_files:
                    try:
                        os.remove(chunk_file)
                    except:
                        pass
                print("Temporary files cleaned up.")
    
    def _combine_transcripts(self, transcripts: List[Dict]) -> str:
        """Combine chunk transcripts into single document with sequential timestamps"""
        combined = []
        
        combined.append("=== COMPLETE VIDEO TRANSCRIPT ===")
        combined.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        combined.append(f"Model: {self.config.model_name}")
        combined.append(f"Prompt: {self.config.prompt_key}")
        combined.append(f"Chunk size: {self.config.chunk_duration_minutes} minutes")
        combined.append(f"Validation: {'Enabled' if self.config.max_retries > 0 else 'Disabled'}")
        combined.append(f"Context continuity: {'Enabled' if len(transcripts) > 1 else 'N/A (single chunk)'}")
        combined.append(f"Sequential timestamps: Enabled")
        combined.append(f"Consensus algorithm: {'Hybrid BERT' if BERT_AVAILABLE else 'Basic'}")
        combined.append("=" * 50)
        combined.append("")
        
        for chunk_data in transcripts:
            chunk_num = chunk_data['chunk_number']
            transcript = chunk_data['transcript'].strip()
            chunk_start_minutes = (chunk_num - 1) * self.config.chunk_duration_minutes
            
            if transcript and not transcript.startswith('[CONSENSUS_FAILED'):
                combined.append(f"--- Chunk {chunk_num} (Starting at {chunk_start_minutes:.1f} minutes) ---")
                
                # Adjust timestamps to be sequential across chunks
                adjusted_transcript = self._adjust_chunk_timestamps(transcript, chunk_start_minutes)
                combined.append(adjusted_transcript)
                combined.append("")
        
        return "\n".join(combined)
    
    def _adjust_chunk_timestamps(self, transcript: str, chunk_start_minutes: float) -> str:
        """Adjust timestamps in chunk transcript to be sequential in full video"""
        lines = transcript.split('\n')
        adjusted_lines = []
        
        # Pattern to match timestamps at start of line (various formats)
        timestamp_pattern = r'^(\d{1,2}:\d{2})\s+(\w+):\s*(.*)$'
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Skip header/metadata lines
            if (line.startswith('===') or line.startswith('---') or 
                line.startswith('Generated:') or line.startswith('Model:') or
                line.startswith('Prompt:') or line.startswith('Chunk size:') or
                line.startswith('Validation:')):
                continue
            
            # Try to parse timestamp line
            match = re.match(timestamp_pattern, line)
            if match:
                original_timestamp = match.group(1)
                speaker = match.group(2)
                content = match.group(3)
                
                # Convert MM:SS to total seconds
                try:
                    time_parts = original_timestamp.split(':')
                    if len(time_parts) == 2:
                        minutes = int(time_parts[0])
                        seconds = int(time_parts[1])
                        chunk_seconds = minutes * 60 + seconds
                        
                        # Add chunk start offset
                        total_seconds = chunk_seconds + (chunk_start_minutes * 60)
                        
                        # Convert back to MM:SS format
                        new_minutes = int(total_seconds // 60)
                        new_seconds = int(total_seconds % 60)
                        new_timestamp = f"{new_minutes:02d}:{new_seconds:02d}"
                        
                        # Reconstruct line with new timestamp
                        adjusted_line = f"{new_timestamp} {speaker}: {content}"
                        adjusted_lines.append(adjusted_line)
                    else:
                        # Keep original line if timestamp format is unexpected
                        adjusted_lines.append(line)
                except (ValueError, IndexError):
                    # Keep original line if timestamp parsing fails
                    adjusted_lines.append(line)
            else:
                # Non-timestamp line, keep as-is
                adjusted_lines.append(line)
        
        return '\n'.join(adjusted_lines)

def main():
    parser = argparse.ArgumentParser(description="Process classroom videos for educational research transcription with Hybrid BERT consensus")
    parser.add_argument("video_path", nargs='?', help="Path to input video file")
    parser.add_argument("-o", "--output", help="Output directory (default: video_name_transcription)")
    parser.add_argument("-c", "--chunk-minutes", type=float, default=3.0, help="Chunk duration in minutes (default: 3.0)")
    parser.add_argument("-m", "--model", default="gemini-2.5-pro-preview-05-06", help="Gemini model to use")
    parser.add_argument("--fps", type=int, default=1, help="Frames per second for video analysis (1-10, default: 1)")
    parser.add_argument("-p", "--prompt", default="basic", help="Prompt to use (default: basic)")
    parser.add_argument("--consensus-runs", type=int, default=1, help="Number of transcription runs per chunk for consensus flagging (default: 1)")
    parser.add_argument("--consensus-threshold", type=float, default=0.7, help="Confidence threshold for auto-accept (default: 0.7)")
    parser.add_argument("--max-retries", type=int, default=3, help="Maximum retry attempts for failed transcriptions (default: 3)")
    parser.add_argument("--retry-delay", type=float, default=5.0, help="Seconds to wait between retries (default: 5.0)")
    parser.add_argument("--min-transcript-length", type=int, default=50, help="Minimum characters for valid transcript (default: 50)")
    parser.add_argument("--fast-chunking", action="store_true", help="Use fast chunking (may have timing issues)")
    parser.add_argument("--disable-repetition-filter", action="store_true", help="Disable post-processing repetition filter")
    parser.add_argument("--list-prompts", action="store_true", help="List available prompts and exit")
    parser.add_argument("--thinking", action="store_true", default=True, help="Enable thinking mode (default: True)")
    parser.add_argument("--no-thinking", action="store_true", help="Disable thinking mode")
    parser.add_argument("--estimate-only", action="store_true", help="Only show cost estimate, don't process")
    parser.add_argument("--api-key", help="Gemini API key (or set GOOGLE_API_KEY env var)")
    
    args = parser.parse_args()
    
    # Handle prompt listing
    if args.list_prompts:
        prompt_manager = PromptManager()
        prompt_manager.list_prompts()
        return
    
    # Require video path if not just listing prompts
    if not args.video_path:
        parser.error("video_path is required unless using --list-prompts")
    
    # Get API key
    api_key = args.api_key or os.getenv('GOOGLE_API_KEY')
    if not api_key:
        print("Please provide API key via --api-key argument or GOOGLE_API_KEY environment variable")
        sys.exit(1)
    
    # Create configuration
    thinking_enabled = not args.no_thinking if args.no_thinking else args.thinking
    config = TranscriptionConfig(
        chunk_duration_minutes=args.chunk_minutes,
        model_name=args.model,
        fps=args.fps,
        thinking=thinking_enabled,
        prompt_key=args.prompt,
        consensus_runs=args.consensus_runs,
        consensus_threshold=args.consensus_threshold,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay,
        min_transcript_length=args.min_transcript_length,
        precise_chunking=not args.fast_chunking,  # Invert: fast_chunking=True means precise_chunking=False
        enable_repetition_filter=not args.disable_repetition_filter  # Invert: disable=True means enable=False
    )
    
    try:
        if args.estimate_only:
            # Just show cost estimate
            duration = VideoChunker.get_video_duration(args.video_path)
            if duration == 0:
                print("Could not determine video duration")
                sys.exit(1)
                
            base_cost_estimate = VideoCostCalculator.estimate_cost(duration, args.model, args.chunk_minutes, args.fps)
            
            # Adjust for consensus runs
            cost_estimate = base_cost_estimate.copy()
            if args.consensus_runs > 1:
                cost_estimate['total_cost'] *= args.consensus_runs
                cost_estimate['input_cost'] *= args.consensus_runs
                cost_estimate['output_cost'] *= args.consensus_runs
                cost_estimate['total_tokens_estimated'] *= args.consensus_runs
            
            consensus_type = "Hybrid BERT" if BERT_AVAILABLE else "Basic"
            print(f"\n=== Cost Estimate ===")
            print(f"Video: {args.video_path}")
            print(f"Duration: {duration:.1f} minutes")
            print(f"Model: {args.model}")
            print(f"Chunk size: {args.chunk_minutes} minutes")
            print(f"FPS setting: {args.fps} frames/second")
            print(f"Thinking mode: {'ON' if thinking_enabled else 'OFF'}")
            print(f"Prompt: {args.prompt}")
            if args.consensus_runs > 1:
                print(f"Consensus flagging: {args.consensus_runs} runs per chunk ({consensus_type} algorithm)")
            print(f"Validation retries: Up to {args.max_retries} retries per failed run")
            print(f"Tokens per second: {base_cost_estimate['tokens_per_second']}")
            print(f"Number of chunks: {cost_estimate['num_chunks']}")
            print(f"Estimated tokens: {cost_estimate['total_tokens_estimated']:,}")
            print(f"Estimated cost: ${cost_estimate['total_cost']:.3f}")
        else:
            # Process video
            processor = TranscriptProcessor(api_key, config)
            result = processor.process_video(args.video_path, args.output)
            
    except KeyboardInterrupt:
        print("\nProcessing interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
